{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphabet_corpus = string.ascii_letters + string.digits\n",
    "alphabet_corpus_size = len(alphabet_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    path = os.path.join('./Data/', filename)\n",
    "    with open(path) as file:\n",
    "        variable = file.readlines()\n",
    "    variable = [x.strip() for x in variable]\n",
    "    return variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location = read_file('location.txt') + read_file('airport_name.txt')\n",
    "company = read_file('company_name.txt')\n",
    "random_string = read_file('random_string.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# location = list(set(location))\n",
    "# company = list(set(company))[:len(location)]\n",
    "# random_string = list(set(random_string))\n",
    "# data = location + company + random_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 10000\n",
    "location = list(set(location))[:size]\n",
    "company = list(set(company))[:size]\n",
    "random_string = list(set(random_string))[:size]\n",
    "data = location + company + random_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_label(category, representation, input_category):\n",
    "    for i in range(len(category)):\n",
    "        if category[i] in input_category:\n",
    "            pass\n",
    "        else:\n",
    "            input_category[category[i]] = representation\n",
    "    return input_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_category = data_label(location, 0, input_category={})\n",
    "input_category = data_label(company, 1, input_category=input_category)\n",
    "input_category = data_label(random_string, 2, input_category=input_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input_category = data_label(random_string, 0, input_category={})\n",
    "# input_category = data_label(company, 1, input_category=input_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped_data = [(name, label) for name, label in input_category.items()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def alphabet_index(alphabet):\n",
    "    if alphabet_corpus.find(alphabet) != -1:\n",
    "        index = alphabet_corpus.find(alphabet)\n",
    "    else:\n",
    "        raise ValueError('Alphabet not in corpus')\n",
    "    return index\n",
    "\n",
    "def alphabet_to_tensor(alphabet):\n",
    "    tensor = torch.zeros(1, alphabet_corpus_size)\n",
    "    try:\n",
    "        tensor[0, alphabet_index(alphabet)] = 1\n",
    "    except:\n",
    "        tensor = torch.zeros(1, alphabet_corpus_size)\n",
    "    return tensor\n",
    "    \n",
    "def word_to_tensor(words):\n",
    "    batch_size = len(words)\n",
    "    max_sequence = max(len(x) for x in words)\n",
    "    tensor = torch.zeros(max_sequence, batch_size, alphabet_corpus_size)\n",
    "    for i, word in enumerate(words):\n",
    "        for j, alphabet in enumerate(word):\n",
    "            tensor[j, i, :] = alphabet_to_tensor(alphabet)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data, test_data = train_test_split(grouped_data, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_batch = DataLoader(training_data, batch_size=256, drop_last=True)\n",
    "test_batch = DataLoader(test_data, batch_size=256, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, \n",
    "                          num_layers=1, nonlinearity='tanh', batch_first=False) \n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        batch_size = input_.shape[1]\n",
    "        hidden = self.first_hidden(batch_size)\n",
    "        hiddens, hidden = self.rnn(input_, hidden)\n",
    "        output = self.linear(hidden)\n",
    "        return output\n",
    "    \n",
    "    def first_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, \n",
    "                          num_layers=1, batch_first=False) \n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        batch_size = input_.shape[1]\n",
    "        hidden = self.first_hidden(batch_size)\n",
    "        hiddens, (hidden, cell_state) = self.lstm(input_, (hidden, hidden))\n",
    "        output = self.linear(hidden)\n",
    "        return output\n",
    "    \n",
    "    def first_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_size = 3\n",
    "hidden_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(word_batch, label_batch):\n",
    "    word_tensor = word_to_tensor(word_batch)\n",
    "    labels = label_batch\n",
    "    optimiser.zero_grad()\n",
    "    # output = rnn(word_tensor)\n",
    "    output = lstm(word_tensor)\n",
    "    loss = criterion(output[0], labels)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation(word_batch, label_batch):\n",
    "    word_tensor = word_to_tensor(word_batch)\n",
    "    labels = label_batch\n",
    "    # output = rnn(word_tensor)\n",
    "    output = lstm(word_tensor)\n",
    "    loss = criterion(output[0], labels)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "validation_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_epoch = 10\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rnn = RNN(alphabet_corpus_size, hidden_size, output_size)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimiser = optim.Adam(rnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = LSTM(alphabet_corpus_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(lstm.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(total_epoch)):\n",
    "    epoch_loss = 0\n",
    "    val_loss = 0\n",
    "    for string, label in training_batch:\n",
    "        # rnn.train()\n",
    "        lstm.train()\n",
    "        loss = train(string, label)\n",
    "        epoch_loss += loss\n",
    "    epoch_loss = epoch_loss/len(training_batch)\n",
    "    train_loss.append(epoch_loss)\n",
    "    \n",
    "    for string, label in test_batch:\n",
    "        with torch.no_grad():\n",
    "            # rnn.eval()\n",
    "            lstm.eval()\n",
    "            loss = validation(string, label)\n",
    "            val_loss += loss\n",
    "    val_loss = val_loss/len(test_batch)\n",
    "    validation_loss.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label='train')\n",
    "plt.plot(validation_loss, label='validation')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(category_number):\n",
    "    return np.zeros((category_number,category_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_score(confusion_matrix):\n",
    "    accuracy = 0\n",
    "    for i in range(len(confusion_matrix)):\n",
    "        accuracy += confusion_matrix[i,i]\n",
    "    accuracy = accuracy/sum(sum(confusionMatrix))*100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_outputs = []\n",
    "actual_outputs = []\n",
    "for string, label in test_batch:\n",
    "    with torch.no_grad():\n",
    "        # rnn.eval()\n",
    "        lstm.eval()\n",
    "        word_tensor = word_to_tensor(string)\n",
    "        # predicted_output = rnn(word_tensor).argmax(axis=2).tolist()[0]\n",
    "        predicted_output = lstm(word_tensor).argmax(axis=2).tolist()[0]\n",
    "        actual_output = label.tolist()\n",
    "        predicted_outputs += predicted_output\n",
    "        actual_outputs += actual_output\n",
    "predicted_outputs = np.array(predicted_outputs)\n",
    "actual_outputs = np.array(actual_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# actual row, predict column\n",
    "confusionMatrix = confusion_matrix(output_size)\n",
    "for i in range(len(actual_outputs)):\n",
    "    if actual_outputs[i] == predicted_outputs[i]:\n",
    "        confusionMatrix[actual_outputs[i],actual_outputs[i]] += 1\n",
    "    elif actual_outputs[i] != predicted_outputs[i]:\n",
    "        confusionMatrix[actual_outputs[i],predicted_outputs[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusionMatrix_norm = confusionMatrix/confusionMatrix.sum(axis=1)\n",
    "sns.heatmap(confusionMatrix_norm, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_score(confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
